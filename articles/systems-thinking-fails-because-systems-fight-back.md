# Systems Thinking Fails Because Systems Fight Back

> Source: https://worksinprogress.co/issue/magical-systems-thinking/

#topic:systems_thinking, #topic:public_policy, #topic:organizational_design, #topic:government, #topic:technology, #topic:engineering, #topic:project_management, #topic:complexity_theory
#issue:healthcare_reform, #issue:covid_19, #issue:energy_infrastructure, #issue:bureaucracy, #issue:ai_development
#sentiment:critical
#people:jay_wright_forrester, #people:john_gall, #people:henri_louis_le_chatelier, #people:bernard_schriever, #people:sundar_pichai, #people:emmanuel_macron, #people:jean_louis_georgelin, #people:linus_pauling

This essay argues that systems thinking—the belief that complex systems can be deliberately designed through careful analysis—fundamentally misunderstands how successful systems emerge. Modern systems like water supplies, the internet, and supply chains evolved from simple working prototypes through iteration, not from master plans. Recent failures like HealthCare.gov and Australia's disability reforms demonstrate that despite sophisticated analytical tools, complex systems designed from scratch consistently fail. Drawing on John Gall's 'Systemantics' and Le Chatelier's Principle from chemistry, the author contends that complex systems inherently resist their intended purposes and kick back against interventions. The key insight is Gall's Law: complex systems that work invariably evolved from simple systems that worked. Successful examples like Operation Warp Speed, Notre-Dame's reconstruction, and Estonia's digital government succeeded by starting small, working outside existing bureaucracies, and iterating gradually. The essay warns against using AI to 'solve' systems design, advocating instead for humility and starting with simple, working systems that can evolve over time.

> The essay's examples are somewhat cherry-picked—there are counterexamples of successful large-scale system designs and failed small-scale approaches that aren't discussed
> The Forrester World Dynamics predictions being wrong doesn't necessarily invalidate all systems modeling—many factors could explain the inaccuracy beyond fundamental methodology flaws
> The dichotomy between 'systems thinking' and 'simple systems' may be oversimplified—successful projects often combine both approaches
> The comparison to AI-generated code percentages at the end seems speculative and not fully developed

---
